{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Supervised Learning with pre-labeled datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "940ea80d455badf7"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"YTSentAnal2\").getOrCreate()\n",
    "\n",
    "# Step 2: Define Dataset Paths\n",
    "file_paths = {\n",
    "    \"LoganPaul\": \"LoganPaul.csv\",\n",
    "    \"OKGO\": \"OKGO.csv\",\n",
    "    \"RoyalWedding\": \"RoyalWedding.csv\",\n",
    "    \"TaylorSwift\": \"TaylorSwift.csv\",\n",
    "    \"Trump\": \"trump.csv\",\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:33.181137Z",
     "start_time": "2024-11-26T00:42:33.153897Z"
    }
   },
   "id": "d268ae8e9b50c1ae"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43b59c5787c84862",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:33.736569Z",
     "start_time": "2024-11-26T00:42:33.172912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|   -1|  780|\n",
      "|    1|  818|\n",
      "|    0| 1238|\n",
      "+-----+-----+\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load and basic cleaning of data\n",
    "def load_and_clean(file_path, delimiter=\",\"):\n",
    "    schema = StructType([\n",
    "        StructField(\"label\", IntegerType(), True),\n",
    "        StructField(\"text\", StringType(), True)\n",
    "    ])\n",
    "    df = spark.read.option(\"header\", \"false\").option(\"sep\", delimiter).schema(schema).csv(file_path)\n",
    "    return df.filter((col(\"text\").isNotNull()) & (col(\"label\").isNotNull()))\n",
    "\n",
    "datasets = [load_and_clean(file_path, delimiter=\";\" if name == \"OKGO\" else \",\") for name, file_path in file_paths.items()]\n",
    "\n",
    "# Combine all datasets into one DataFrame\n",
    "combined_df = spark.createDataFrame([], schema=datasets[0].schema)\n",
    "for df in datasets:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "# Verify label column distribution\n",
    "combined_df.groupBy(\"label\").count().show()\n",
    "\n",
    "# Check schema to confirm proper column names\n",
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text Normalization with NLTK\n",
    "\n",
    "This section tackles more preprocessing of text data, transforming raw text into meaningful tokens. Leveraging NLTK, we remove noise such as special characters, digits, and common stopwords. Additionally, we apply lemmatization to reduce words to their base form, ensuring consistency in representation. The use of PySpark's UDF (User-Defined Functions) enables seamless integration of Python-based transformations into the distributed processing pipeline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbe96cae8e7ac43b"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfabefc793210c71",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:39.395892Z",
     "start_time": "2024-11-26T00:42:33.739497Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ammaar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ammaar/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ammaar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filtered_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: integer (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3196:============================>                           (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|filtered_tokens                                                                                                                       |label|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[wow, heard, guy, easily, insecure, douche, ever, seen, youtube, clearly, mental, issue, need, evaluated, give, guy, help, need, asap]|1    |\n",
      "|[japanese, trying, respectful, lo, gan, logan, care, wtf]                                                                             |-1   |\n",
      "|[prick]                                                                                                                               |-1   |\n",
      "|[think, weed, cry]                                                                                                                    |-1   |\n",
      "|[lmao, american, comment, section, acting, like, nuked, japan, third, time]                                                           |0    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------+-----+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define preprocessing UDF\n",
    "def preprocess_text(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        word = re.sub(r'[^\\w\\s]', '', word)  # Remove special characters\n",
    "        word = re.sub(r'\\d+', '', word)  # Remove digits\n",
    "        if word not in stop_words:  # Remove stopwords\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)  # Lemmatize word\n",
    "            processed_tokens.append(lemmatized_word)\n",
    "    return processed_tokens\n",
    "\n",
    "preprocess_udf = udf(preprocess_text, ArrayType(StringType()))\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized_df = tokenizer.transform(combined_df)\n",
    "\n",
    "# Apply UDF for normalization and lemmatization\n",
    "normalized_df = tokenized_df.withColumn(\n",
    "    \"filtered_tokens\", preprocess_udf(col(\"tokens\"))\n",
    ")\n",
    "\n",
    "# Filter out empty tokenized rows\n",
    "normalized_df = normalized_df.filter(col(\"filtered_tokens\").isNotNull())\n",
    "\n",
    "# Select only required columns\n",
    "refined_preprocessed_df = normalized_df.select(\"filtered_tokens\", \"label\")\n",
    "\n",
    "# Verify the updated normalization\n",
    "refined_preprocessed_df.printSchema()\n",
    "refined_preprocessed_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Balancing the Dataset with Class Weights\n",
    "\n",
    "Since we know these datasets exhibit imbalanced class distributions (-1:780, 0:818, 1: 1238), which can bias the model. We can address this by calculating class weights proportional to their frequencies. These weights are added to the dataset, ensuring fairer representation of each sentiment class during model training. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da9398aae7bac7e6"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5155ae8f50056a1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:42.779414Z",
     "start_time": "2024-11-26T00:42:39.395186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3197:======================================>                (9 + 4) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|         avg(weight)|\n",
      "+-----+--------------------+\n",
      "|   -1|0.001282051282051...|\n",
      "|    1|8.077544426494289E-4|\n",
      "|    0|0.001222493887530...|\n",
      "+-----+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Class Weighting\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Add weights to the dataset\n",
    "class_weights = {\n",
    "    -1: 1.0 / 780,  \n",
    "    0: 1.0 / 818,\n",
    "    1: 1.0 / 1238\n",
    "}\n",
    "\n",
    "# Add weights to the dataset\n",
    "balanced_training_data = refined_preprocessed_df.withColumn(\n",
    "    \"weight\",\n",
    "    when(col(\"label\") == -1, class_weights[-1])\n",
    "    .when(col(\"label\") == 0, class_weights[0])\n",
    "    .when(col(\"label\") == 1, class_weights[1])\n",
    ")\n",
    "\n",
    "# Verify the added weights\n",
    "balanced_training_data.groupBy(\"label\").agg({\"weight\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Extraction with CountVectorizer and IDF\n",
    "Next, we transition from raw tokens to numerical feature vectors. Using `CountVectorizer`, we create a vocabulary-based representation of the text, capturing the frequency of terms. The `IDF` (Inverse Document Frequency) scaling step enhances the representation by down-weighting common terms, emphasizing words more indicative of the sentiment. These stages are encapsulated in a pipeline for streamlined preprocessing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebeea9405a8e9b9f"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3d85608ba6291e78",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:55.973304Z",
     "start_time": "2024-11-26T00:42:42.694458Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3208:===================================>                    (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                       |label|weight               |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------+\n",
      "|(4538,[8,38,41,55,115,124,139,163,282,513,633,657,930,984,1760,1967,2160],[6.165935968706538,3.961518388244577,8.03735360416905,4.236930368104543,4.815008218879702,4.859459981450535,5.117289090752635,5.006063455642411,5.465595785020851,6.158742965580796,6.341064522374751,6.341064522374751,6.5642080736889605,6.5642080736889605,7.257355254248906,7.257355254248906,7.257355254248906])|1    |8.077544426494346E-4 |\n",
      "|(4538,[19,74,132,155,285,322,777,2394],[3.5560532801364126,4.516515230323705,4.859459981450535,4.95477016125486,5.465595785020851,5.552607162010481,6.341064522374751,7.257355254248906])                                                                                                                                                                                                      |-1   |0.001282051282051282 |\n",
      "|(4538,[1610],[6.851890146140741])                                                                                                                                                                                                                                                                                                                                                              |-1   |0.001282051282051282 |\n",
      "|(4538,[26,502,4436],[3.7608476927824257,6.004592285753538,7.257355254248906])                                                                                                                                                                                                                                                                                                                  |-1   |0.001282051282051282 |\n",
      "|(4538,[4,24,32,84,106,310,625,1779,3003,3500],[2.6422347374076467,3.716395930211592,3.9802105212567294,4.453994873342371,4.69240589678737,5.552607162010481,6.5642080736889605,7.257355254248906,7.257355254248906,7.257355254248906])                                                                                                                                                         |0    |0.0012224938875305623|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\", vocabSize=10000)\n",
    "\n",
    "# Apply IDF for scaling feature vectors\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Create a feature extraction pipeline\n",
    "feature_pipeline = Pipeline(stages=[count_vectorizer, idf])\n",
    "\n",
    "# Fit and transform the pipeline on the weighted dataset\n",
    "feature_model = feature_pipeline.fit(balanced_training_data)\n",
    "featured_df = feature_model.transform(balanced_training_data)\n",
    "\n",
    "# Select necessary columns for training\n",
    "final_training_data = featured_df.select(\"features\", \"label\", \"weight\")\n",
    "\n",
    "# Verify the resulting dataset\n",
    "final_training_data.printSchema()\n",
    "final_training_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Label Remapping\n",
    "\n",
    "Since PySparkâ€™s MLlib LogisticRegression does not support negative label values and since with is a multiclass classification, we need to remap the labels (-1, 0, 1) to non-negative integers (0, 1, 2). This transformation ensures compatibility with Spark's MLlib while preserving the underlying semantics of the labels."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df461bf53b234e99"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6ce7f88d8ca1e1c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:56.104061Z",
     "start_time": "2024-11-26T00:42:55.972231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label Remapping and Data Splitting\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Remap labels: -1 -> 0, 0 -> 1, 1 -> 2 for ML since -1 cannot b used 4 training\n",
    "final_training_data = final_training_data.withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"label\") == -1, 0)\n",
    "    .when(col(\"label\") == 0, 1)\n",
    "    .when(col(\"label\") == 1, 2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the Logistic Regression Model\n",
    "The logistic regression model, optimized using k-fold cross-validation. A parameter grid is defined to tune regularization strength (`RegParam`) and the ElasticNet mixing parameter. The cross-validation ensures robust model evaluation across multiple splits, providing the best hyperparameters for the task. Finally, the best model is saved for reuse, with its key parameters displayed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1579107323864514"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53d308b606fc82d7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:49:16.216464Z",
     "start_time": "2024-11-26T00:47:33.378059Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: RegParam=0.01, ElasticNetParam=0.5\n"
     ]
    }
   ],
   "source": [
    "# Log regression model training\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define Logistic Regression model with weights\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\", maxIter=20)\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Initialize k-fold CrossValidator\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "    numFolds=5  # 5-fold cross-validation\n",
    ")\n",
    "\n",
    "# Train Logistic Regression with cross-validation\n",
    "cv_model = cross_validator.fit(final_training_data)\n",
    "\n",
    "# Extract the best model\n",
    "best_model = cv_model.bestModel\n",
    "print(f\"Best Model Parameters: RegParam={best_model._java_obj.getRegParam()}, ElasticNetParam={best_model._java_obj.getElasticNetParam()}\")\n",
    "\n",
    "# Save the best model for reuse\n",
    "best_model.write().overwrite().save(\"logistic_regression_best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac8601fb11f4d086",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:49:57.119044Z",
     "start_time": "2024-11-26T00:49:26.609535Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated Test Accuracy: 0.900916784203103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Precision = 0.9605633802816902, Recall = 0.8743589743589744, F1-Score = 0.9154362416107382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: Precision = 0.8342541436464088, Recall = 0.975767366720517, F1-Score = 0.8994787788533134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8915:=============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 2: Precision = 0.9808259587020649, Recall = 0.812958435207824, F1-Score = 0.8890374331550803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the entire dataset\n",
    "test_predictions = best_model.transform(final_training_data)\n",
    "\n",
    "# Evaluate test accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "print(f\"Cross-Validated Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Evaluate precision, recall, and F1-score for each class\n",
    "labels = [0, 1, 2]  \n",
    "for label in labels:\n",
    "    precision = evaluator.evaluate(test_predictions, {evaluator.metricName: \"precisionByLabel\", evaluator.metricLabel: label})\n",
    "    recall = evaluator.evaluate(test_predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: label})\n",
    "    f1 = evaluator.evaluate(test_predictions, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: label})\n",
    "    print(f\"Class {label}: Precision = {precision}, Recall = {recall}, F1-Score = {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "876b90e2d15ca201",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:52:49.032659Z",
     "start_time": "2024-11-26T00:52:41.983996Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Label 0 Predicted as 0: 682\n",
      "Label 0 Predicted as 1: 95\n",
      "Label 0 Predicted as 2: 3\n",
      "Label 1 Predicted as 0: 20\n",
      "Label 1 Predicted as 1: 1208\n",
      "Label 1 Predicted as 2: 10\n",
      "Label 2 Predicted as 0: 8\n",
      "Label 2 Predicted as 1: 145\n",
      "Label 2 Predicted as 2: 665\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Create a confusion matrix DataFrame\n",
    "confusion_matrix_df = (\n",
    "    test_predictions\n",
    "    .groupBy(\"label\", \"prediction\")\n",
    "    .agg(F.count(\"*\").alias(\"count\"))\n",
    "    .orderBy(\"label\", \"prediction\")\n",
    ")\n",
    "\n",
    "# Collect the confusion matrix for display\n",
    "confusion_matrix = confusion_matrix_df.collect()\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "for row in confusion_matrix:\n",
    "    print(f\"Label {int(row['label'])} Predicted as {int(row['prediction'])}: {row['count']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
