{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d268ae8e9b50c1ae",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:33.181137Z",
     "start_time": "2024-11-26T00:42:33.153897Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"YTSentAnal2\").getOrCreate()\n",
    "\n",
    "# Step 2: Define Dataset Paths\n",
    "file_paths = {\n",
    "    \"LoganPaul\": \"LoganPaul.csv\",\n",
    "    \"OKGO\": \"OKGO.csv\",\n",
    "    \"RoyalWedding\": \"RoyalWedding.csv\",\n",
    "    \"TaylorSwift\": \"TaylorSwift.csv\",\n",
    "    \"Trump\": \"trump.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43b59c5787c84862",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:33.736569Z",
     "start_time": "2024-11-26T00:42:33.172912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|   -1|  780|\n",
      "|    1|  818|\n",
      "|    0| 1238|\n",
      "+-----+-----+\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load and clean data\n",
    "def load_and_clean(file_path, delimiter=\",\"):\n",
    "    schema = StructType([\n",
    "        StructField(\"label\", IntegerType(), True),\n",
    "        StructField(\"text\", StringType(), True)\n",
    "    ])\n",
    "    df = spark.read.option(\"header\", \"false\").option(\"sep\", delimiter).schema(schema).csv(file_path)\n",
    "    return df.filter((col(\"text\").isNotNull()) & (col(\"label\").isNotNull()))\n",
    "\n",
    "datasets = [load_and_clean(file_path, delimiter=\";\" if name == \"OKGO\" else \",\") for name, file_path in file_paths.items()]\n",
    "\n",
    "# Combine all datasets into one DataFrame\n",
    "combined_df = spark.createDataFrame([], schema=datasets[0].schema)\n",
    "for df in datasets:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "# Verify label column distribution\n",
    "combined_df.groupBy(\"label\").count().show()\n",
    "\n",
    "# Check schema to confirm proper column names\n",
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfabefc793210c71",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:39.395892Z",
     "start_time": "2024-11-26T00:42:33.739497Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ammaar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ammaar/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ammaar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filtered_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: integer (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3196:============================>                           (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|filtered_tokens                                                                                                                       |label|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[wow, heard, guy, easily, insecure, douche, ever, seen, youtube, clearly, mental, issue, need, evaluated, give, guy, help, need, asap]|1    |\n",
      "|[japanese, trying, respectful, lo, gan, logan, care, wtf]                                                                             |-1   |\n",
      "|[prick]                                                                                                                               |-1   |\n",
      "|[think, weed, cry]                                                                                                                    |-1   |\n",
      "|[lmao, american, comment, section, acting, like, nuked, japan, third, time]                                                           |0    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------+-----+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Text Normalization with NLTK\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define preprocessing UDF\n",
    "def preprocess_text(tokens):\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        word = re.sub(r'[^\\w\\s]', '', word)  # Remove special characters\n",
    "        word = re.sub(r'\\d+', '', word)  # Remove digits\n",
    "        if word not in stop_words:  # Remove stopwords\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)  # Lemmatize word\n",
    "            processed_tokens.append(lemmatized_word)\n",
    "    return processed_tokens\n",
    "\n",
    "preprocess_udf = udf(preprocess_text, ArrayType(StringType()))\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized_df = tokenizer.transform(combined_df)\n",
    "\n",
    "# Apply UDF for normalization and lemmatization\n",
    "normalized_df = tokenized_df.withColumn(\n",
    "    \"filtered_tokens\", preprocess_udf(col(\"tokens\"))\n",
    ")\n",
    "\n",
    "# Filter out empty tokenized rows\n",
    "normalized_df = normalized_df.filter(col(\"filtered_tokens\").isNotNull())\n",
    "\n",
    "# Select only required columns\n",
    "refined_preprocessed_df = normalized_df.select(\"filtered_tokens\", \"label\")\n",
    "\n",
    "# Verify the updated normalization\n",
    "refined_preprocessed_df.printSchema()\n",
    "refined_preprocessed_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5155ae8f50056a1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:42.779414Z",
     "start_time": "2024-11-26T00:42:39.395186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3197:======================================>                (9 + 4) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|         avg(weight)|\n",
      "+-----+--------------------+\n",
      "|   -1|0.001282051282051...|\n",
      "|    1|8.077544426494289E-4|\n",
      "|    0|0.001222493887530...|\n",
      "+-----+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Class Weighting\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Add weights to the dataset\n",
    "class_weights = {\n",
    "    -1: 1.0 / 780,  \n",
    "    0: 1.0 / 818,\n",
    "    1: 1.0 / 1238\n",
    "}\n",
    "\n",
    "# Add weights to the dataset\n",
    "balanced_training_data = refined_preprocessed_df.withColumn(\n",
    "    \"weight\",\n",
    "    when(col(\"label\") == -1, class_weights[-1])\n",
    "    .when(col(\"label\") == 0, class_weights[0])\n",
    "    .when(col(\"label\") == 1, class_weights[1])\n",
    ")\n",
    "\n",
    "# Verify the added weights\n",
    "balanced_training_data.groupBy(\"label\").agg({\"weight\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3d85608ba6291e78",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:55.973304Z",
     "start_time": "2024-11-26T00:42:42.694458Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3208:===================================>                    (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                       |label|weight               |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------+\n",
      "|(4538,[8,38,41,55,115,124,139,163,282,513,633,657,930,984,1760,1967,2160],[6.165935968706538,3.961518388244577,8.03735360416905,4.236930368104543,4.815008218879702,4.859459981450535,5.117289090752635,5.006063455642411,5.465595785020851,6.158742965580796,6.341064522374751,6.341064522374751,6.5642080736889605,6.5642080736889605,7.257355254248906,7.257355254248906,7.257355254248906])|1    |8.077544426494346E-4 |\n",
      "|(4538,[19,74,132,155,285,322,777,2394],[3.5560532801364126,4.516515230323705,4.859459981450535,4.95477016125486,5.465595785020851,5.552607162010481,6.341064522374751,7.257355254248906])                                                                                                                                                                                                      |-1   |0.001282051282051282 |\n",
      "|(4538,[1610],[6.851890146140741])                                                                                                                                                                                                                                                                                                                                                              |-1   |0.001282051282051282 |\n",
      "|(4538,[26,502,4436],[3.7608476927824257,6.004592285753538,7.257355254248906])                                                                                                                                                                                                                                                                                                                  |-1   |0.001282051282051282 |\n",
      "|(4538,[4,24,32,84,106,310,625,1779,3003,3500],[2.6422347374076467,3.716395930211592,3.9802105212567294,4.453994873342371,4.69240589678737,5.552607162010481,6.5642080736889605,7.257355254248906,7.257355254248906,7.257355254248906])                                                                                                                                                         |0    |0.0012224938875305623|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\", vocabSize=10000)\n",
    "\n",
    "# Apply IDF for scaling feature vectors\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Create a feature extraction pipeline\n",
    "feature_pipeline = Pipeline(stages=[count_vectorizer, idf])\n",
    "\n",
    "# Fit and transform the pipeline on the weighted dataset\n",
    "feature_model = feature_pipeline.fit(balanced_training_data)\n",
    "featured_df = feature_model.transform(balanced_training_data)\n",
    "\n",
    "# Select necessary columns for training\n",
    "final_training_data = featured_df.select(\"features\", \"label\", \"weight\")\n",
    "\n",
    "# Verify the resulting dataset\n",
    "final_training_data.printSchema()\n",
    "final_training_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6ce7f88d8ca1e1c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:42:56.104061Z",
     "start_time": "2024-11-26T00:42:55.972231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label Remapping and Data Splitting\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Remap labels: -1 -> 0, 0 -> 1, 1 -> 2 for ML since -1 cannot b used 4 training\n",
    "final_training_data = final_training_data.withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"label\") == -1, 0)\n",
    "    .when(col(\"label\") == 0, 1)\n",
    "    .when(col(\"label\") == 1, 2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53d308b606fc82d7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:49:16.216464Z",
     "start_time": "2024-11-26T00:47:33.378059Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: RegParam=0.01, ElasticNetParam=0.5\n"
     ]
    }
   ],
   "source": [
    "# Log regression model training\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define Logistic Regression model with weights\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\", maxIter=20)\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Initialize k-fold CrossValidator\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "    numFolds=5  # 5-fold cross-validation\n",
    ")\n",
    "\n",
    "# Train Logistic Regression with cross-validation\n",
    "cv_model = cross_validator.fit(final_training_data)\n",
    "\n",
    "# Extract the best model\n",
    "best_model = cv_model.bestModel\n",
    "print(f\"Best Model Parameters: RegParam={best_model._java_obj.getRegParam()}, ElasticNetParam={best_model._java_obj.getElasticNetParam()}\")\n",
    "\n",
    "# Save the best model for reuse\n",
    "best_model.write().overwrite().save(\"logistic_regression_best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac8601fb11f4d086",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:49:57.119044Z",
     "start_time": "2024-11-26T00:49:26.609535Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated Test Accuracy: 0.900916784203103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Precision = 0.9605633802816902, Recall = 0.8743589743589744, F1-Score = 0.9154362416107382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: Precision = 0.8342541436464088, Recall = 0.975767366720517, F1-Score = 0.8994787788533134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8915:=============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 2: Precision = 0.9808259587020649, Recall = 0.812958435207824, F1-Score = 0.8890374331550803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the entire dataset\n",
    "test_predictions = best_model.transform(final_training_data)\n",
    "\n",
    "# Evaluate test accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "print(f\"Cross-Validated Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Evaluate precision, recall, and F1-score for each class\n",
    "labels = [0, 1, 2]  \n",
    "for label in labels:\n",
    "    precision = evaluator.evaluate(test_predictions, {evaluator.metricName: \"precisionByLabel\", evaluator.metricLabel: label})\n",
    "    recall = evaluator.evaluate(test_predictions, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: label})\n",
    "    f1 = evaluator.evaluate(test_predictions, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: label})\n",
    "    print(f\"Class {label}: Precision = {precision}, Recall = {recall}, F1-Score = {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "876b90e2d15ca201",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:52:49.032659Z",
     "start_time": "2024-11-26T00:52:41.983996Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Label 0 Predicted as 0: 682\n",
      "Label 0 Predicted as 1: 95\n",
      "Label 0 Predicted as 2: 3\n",
      "Label 1 Predicted as 0: 20\n",
      "Label 1 Predicted as 1: 1208\n",
      "Label 1 Predicted as 2: 10\n",
      "Label 2 Predicted as 0: 8\n",
      "Label 2 Predicted as 1: 145\n",
      "Label 2 Predicted as 2: 665\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Create a confusion matrix DataFrame\n",
    "confusion_matrix_df = (\n",
    "    test_predictions\n",
    "    .groupBy(\"label\", \"prediction\")\n",
    "    .agg(F.count(\"*\").alias(\"count\"))\n",
    "    .orderBy(\"label\", \"prediction\")\n",
    ")\n",
    "\n",
    "# Collect the confusion matrix for display\n",
    "confusion_matrix = confusion_matrix_df.collect()\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "for row in confusion_matrix:\n",
    "    print(f\"Label {int(row['label'])} Predicted as {int(row['prediction'])}: {row['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f9bb37229b33af6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
